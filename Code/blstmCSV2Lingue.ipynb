{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"blstmCSV2Lingue.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xbke5ZvTyawq","executionInfo":{"status":"ok","timestamp":1622471878402,"user_tz":-120,"elapsed":52349,"user":{"displayName":"GAETANO CASILLO","photoUrl":"","userId":"12560129285216303774"}},"outputId":"4fe0d6fb-014f-43d3-e5e6-097865a9f25b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vbWDrgrXymzo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622471917750,"user_tz":-120,"elapsed":6695,"user":{"displayName":"GAETANO CASILLO","photoUrl":"","userId":"12560129285216303774"}},"outputId":"6ad5c4af-bf91-4253-8add-caf5218e2530"},"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sstuAC41zPOW","executionInfo":{"status":"ok","timestamp":1622472607740,"user_tz":-120,"elapsed":238,"user":{"displayName":"GAETANO CASILLO","photoUrl":"","userId":"12560129285216303774"}}},"source":["filename = ''\n","path_drive2 = '/content/drive/MyDrive/Casillo&Natale/dataset/DatasetDueLingue'\n","dataset_dir = ''\n","LANGUAGES = {\n","  1:'Italiano',\n","  2:'Inglese',\n","  3: 'Tedesco',\n","  4:'Spagnolo',\n","  5: 'Olandese',\n","  6:'Russo',\n","  7: 'Giapponese'}\n","\n","combinations = []\n","lang = list(LANGUAGES.keys())\n","for i in range(len(lang)):\n","  for j in range(i+1, len(lang)):\n","    combinations.append((i+1, j+1))"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hfEhwyS4znDX"},"source":["# Caricamento dei dati"]},{"cell_type":"code","metadata":{"id":"27ecV5KHzlzf","executionInfo":{"status":"ok","timestamp":1622472632233,"user_tz":-120,"elapsed":233,"user":{"displayName":"GAETANO CASILLO","photoUrl":"","userId":"12560129285216303774"}}},"source":["# load dataset\n","from numpy import mean\n","from numpy import std\n","from numpy import dstack\n","from pandas import read_csv\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import LSTM, Bidirectional\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot\n","from sklearn.preprocessing import MinMaxScaler\n","import os\n","\n","# load a single file as a numpy array\n","def load_file(filepath):\n","    dataframe = read_csv(filepath, header= None)\n","    return dataframe.values\n","\n","# load a list of files and return as a 3d numpy array\n","def load_group(filenames, prefix=''):\n","    scaler = MinMaxScaler(feature_range=(-1, 1))\n","    loaded = list()\n","    for name in filenames:\n","        data = read_csv(prefix+name)\n","        data = scaler.fit_transform(data)\n","        loaded.append(data)\n","    # stack group so that features are the 3rd dimension\n","    loaded = dstack(loaded)\n","    return loaded\n","\n","# load a dataset group, such as train or test\n","def load_dataset_group(group, prefix= ''):\n","    filepath = path_drive2+'/'+dataset_dir+'/features/'\n","    os.chdir(os.path.join(filepath, group))\n","    filenames = []\n","    filenames = [group+'/feature'+str(i)+'.csv' for i in range(0,66)]\n","    # load input data\n","    X= load_group(filenames, filepath)\n","    #load y \n","    os.chdir(os.path.join(path_drive2, dataset_dir, 'csv'))\n","    y = read_csv(filename+'_'+group.split('_')[0]+'_targets_1.csv')\n","    # load class output\n","    return X,y\n","\n","# load the dataset, returns train and test X and y elements\n","def load_dataset(prefix='', LANGUAGES_N = {}):\n","    # load all train\n","    trainX, trainy = load_dataset_group('train', prefix)\n","    # load all test\n","    testX, testy = load_dataset_group('test', prefix)\n","    validationX, validationy = load_dataset_group('validation', prefix)\n","    trainy['language'] = trainy['language'].map(LANGUAGES_N)\n","    testy['language'] = testy['language'].map(LANGUAGES_N)\n","    validationy['language'] = validationy['language'].map(LANGUAGES_N)\n","    trainy = to_categorical(trainy, len(LANGUAGES_N))\n","    testy = to_categorical(testy, len(LANGUAGES_N))\n","    validationy = to_categorical(validationy, len(LANGUAGES_N))\n","    print(trainX.shape, trainy.shape, testX.shape, testy.shape, validationX.shape, validationy.shape)\n","    return trainX, trainy, testX, testy, validationX, validationy"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IWDGDSJfzrI-"},"source":["# Definizione del modello"]},{"cell_type":"code","metadata":{"id":"CEWAijZBzql_","executionInfo":{"status":"ok","timestamp":1622473266952,"user_tz":-120,"elapsed":225,"user":{"displayName":"GAETANO CASILLO","photoUrl":"","userId":"12560129285216303774"}}},"source":["import tensorflow as tf\n","from keras.callbacks import EarlyStopping\n","\n","verbose, epochs, batch_size = 0, 100, 64\n","def create_model():\n","    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], len(LANGUAGES_N.keys())\n","    # Initialising the RNN\n","    model = Sequential()\n","    model.add(Bidirectional(LSTM(units = 100, return_sequences = True, input_shape = (n_timesteps, n_features))))\n","    model.add(Dropout(0.5))\n","\n","    # Adding a second LSTM layer and Dropout layer\n","    model.add(Bidirectional(LSTM(units = 100, return_sequences = True)))\n","    model.add(Dropout(0.5))\n","\n","    # Adding a third LSTM layer and Dropout layer\n","    model.add(Bidirectional(LSTM(units = 100)))\n","    model.add(Dropout(0.5))\n","\n","    # Adding the output layer\n","    model.add(Dense(n_outputs, activation='softmax'))\n","    \n","    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n","    model.compile(\n","      loss='categorical_crossentropy',\n","      optimizer=opt,\n","      metrics=['accuracy']\n","    )\n","\n","    return model\n","    "],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"I4dtVyUc0I0X"},"source":["from sklearn.metrics import classification_report\n","import pandas as pd\n","import numpy as np\n","\n","with tf.device('/device:GPU:0'):\n","  for combination in combinations[:1]: \n","    dataset_dir = 'dataset_'+str(combination[0])+'_'+str(combination[1])\n","    filename=str(combination[0])+'_'+str(combination[1])\n","    LANGUAGES_N = {\n","        combination[0]: 0,\n","        combination[1]: 1\n","    }\n","    trainX, trainy, testX, testy, validationX, validationy = load_dataset(path_drive2+'/'+dataset_dir, LANGUAGES_N)\n","    tf.random.set_seed(42)\n","    model = create_model()\n","    history = model.fit(trainX, trainy, epochs = epochs, batch_size = batch_size, verbose = verbose, validation_data = (validationX, validationy)) \n","    model.evaluate(testX, testy)\n","    predictions = model.predict_classes(testX) \n","    \n","    LAN = {\n","        combination[0]: LANGUAGES[combination[0]],\n","        combination[1]: LANGUAGES[combination[1]]\n","    }\n","   \n","    testy = np.argmax(testy, axis=1)\n","    target_names = list(LAN.keys())\n","    report = classification_report(testy, predictions, output_dict=True, target_names=target_names)\n","    df = pd.DataFrame(report).transpose()\n","    print (df)\n","    df.to_csv('/content/drive/MyDrive/Casillo&Natale/dataset/Risultati/risultati'+filename+'.csv')"],"execution_count":null,"outputs":[]}]}